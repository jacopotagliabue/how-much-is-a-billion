{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Much is a Billion Dollars?\n",
    "\n",
    "This notebook has been prepared in connection to Jacopo Tagliabue's post \"How Much is a Billion Dollars?\". The following notebook is shared as some hacky-code that gets the job (e.g. get facts from DBPedia) done ;-) \n",
    "\n",
    "This notebook should run smoothly on a standard Python 3 Anaconda kernel + networkx: please refer to the full blog post (and the README of this repo) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globals and import and utils stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import log\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "import networkx as nx   # https://networkx.github.io/documentation/networkx-1.10/tutorial/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data folder from relative position of this notebook and setup the path to dbpedia files\n",
    "DATA_FOLDER = '{}/data'.format(os.path.abspath('..')) \n",
    "print(\"Data is in {}\".format(DATA_FOLDER))\n",
    "DB_LITERALS_FILE = '{}/mappingbased_literals_en.tql'.format(DATA_FOLDER)\n",
    "DB_TRIPLE_FILE = '{}/mappingbased_objects_en.tql'.format(DATA_FOLDER)\n",
    "JSON_OUTPUT_FILE ='{}/knowledge_base.json'.format(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse DBpedia properties\n",
    "We first parse DBPedia properties from the ontology file - since \"range\" and \"domain\" in the RDF collection may be misguided, we rely on string matching to extract a starting subset of _numerical_ properties from a list of units of measure (needless to say, the logic here could be massively improved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dbpedia_triple(line):\n",
    "    match_obj = re.match( r'<(.*?)> <(.*?)> \\\"(.*?)>.', line)\n",
    "    if match_obj:\n",
    "        return [match_obj.group(_) for _ in range(1, 4)]\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line = '<http://dbpedia.org/resource/Audi> <http://dbpedia.org/ontology/assets> \"1.6832E10\"^^<http://dbpedia.org/datatype/euro> <http://en.wikipedia.org/wiki/Audi?oldid=744301837#absolute-line=28&template=Infobox_company&property=assets&split=1&wikiTextSize=62&plainTextSize=22&valueSize=44> .'\n",
    "parse_dbpedia_triple(test_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dbpedia_ontology(line):\n",
    "    match_obj = re.match( r'<(.*?)> <(.*?)> <(.*?)>.', line)\n",
    "    if match_obj:\n",
    "        return [match_obj.group(_) for _ in range(1, 4)]\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_line_2 = '<http://dbpedia.org/resource/Anarchism> <http://www.w3.org/2000/01/rdf-schema#seeAlso> <http://dbpedia.org/resource/Anarchist_terminology> <http://en.wikipedia.org/wiki/Anarchism?oldid=744318951#section=Etymology_and_terminolog&relative-line=2&absolute-line=23&template=Related_articles&property=1&split=1&wikiTextSize=21&plainTextSize=21&valueSize=21> .'\n",
    "parse_dbpedia_ontology(test_line_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dbpedia_value(val):\n",
    "    val = val.split('\"')[0].strip()\n",
    "    # if date\n",
    "    if '-' in val:\n",
    "        return float(val) if val[0] == '-' else float(val.split('-')[0])\n",
    "    # if num\n",
    "    return float(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_dbpedia_value(parse_dbpedia_triple(test_line)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting frequent properties with units of measure that interest us (e.g. seconds, km2, $, etc.), we compiled a list of DBPedia properties to guide our fact extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_PROPERTIES = { \n",
    "    'http://dbpedia.org/ontology/runtime': {\n",
    "        'p': 'http://dbpedia.org/ontology/runtime',\n",
    "        'd': 'runtime',\n",
    "        'u': 'y', # originally these are seconds, so conversion is needed\n",
    "        't': lambda x: x / (60.0 * 60.0 * 24.0 * 365.0),\n",
    "        'm': lambda x: x < 100\n",
    "    },\n",
    "    'http://dbpedia.org/ontology/populationDensity': {\n",
    "        'p': 'http://dbpedia.org/ontology/populationDensity',\n",
    "        'd': 'population density',\n",
    "        'u': 'person/km2',\n",
    "        't': None,\n",
    "        'm': None\n",
    "    },\n",
    "    'http://dbpedia.org/ontology/populationTotal': {\n",
    "        'p': 'http://dbpedia.org/ontology/populationTotal',\n",
    "        'd': 'people',\n",
    "        'u': 'person',\n",
    "        't': None,\n",
    "        'm': lambda x: x < 10000000.0\n",
    "    },\n",
    "    'http://dbpedia.org/ontology/areaTotal': {\n",
    "        'p': 'http://dbpedia.org/ontology/areaTotal',\n",
    "        'd': 'total area',\n",
    "        'u': 'km2', # originally these are m2, so conversion is needed\n",
    "        't': lambda x: x / 1000000.0,\n",
    "        'm': None\n",
    "    },\n",
    "    'http://dbpedia.org/ontology/careerPrizeMoney': {\n",
    "        'p': 'http://dbpedia.org/ontology/careerPrizeMoney',\n",
    "        'd': 'career money',\n",
    "        'u': '$',\n",
    "        't': None,\n",
    "        'm': lambda x: x < 10000000\n",
    "    },\n",
    "    'http://dbpedia.org/ontology/tuition': {\n",
    "        'p': 'http://dbpedia.org/ontology/tuition',\n",
    "        'd': 'tuition',\n",
    "        'u': '$/y',\n",
    "        't': None,\n",
    "        'm': lambda x: x < 1000000 and x > 100\n",
    "    },\n",
    "    'http://dbpedia.org/ontology/networth': {\n",
    "        'p': 'http://dbpedia.org/ontology/networth',\n",
    "        'd': 'net worth',\n",
    "        'u': '$',\n",
    "        't': None,\n",
    "        'm': lambda x: x < 10000000 and x > 1000\n",
    "    },\n",
    "    'http://dbpedia.org/ontology/salary': {\n",
    "        'p': 'http://dbpedia.org/ontology/salary',\n",
    "        'd': 'salary',\n",
    "        'u': '$/y',\n",
    "        't': None,\n",
    "        'm': lambda x: x < 500000 and x > 100\n",
    "    },\n",
    "    'http://dbpedia.org/ontology/foundingDate': {\n",
    "        'p': 'http://dbpedia.org/ontology/foundingDate',\n",
    "        'd': 'years of history',\n",
    "        'u': 'y', # originally this is a date, so conversion is needed\n",
    "        't': lambda x: datetime.today().year - x,\n",
    "        'm': lambda x: x < 100\n",
    "    }   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect mapping resource to name (as label) for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri2name = {}\n",
    "with open(DB_LITERALS_FILE) as my_triples:\n",
    "    for line in my_triples:\n",
    "        elements = parse_dbpedia_triple(line)\n",
    "        if elements and elements[1] == 'http://xmlns.com/foaf/0.1/name':\n",
    "            uri2name[elements[0]] = elements[2].split('\"')[0].strip()\n",
    "            \n",
    "len(uri2name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally parse the triple store SUB-PROP-OBJ to induce a graph over DBPedia and calculate PageRank over entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_graph = nx.DiGraph()\n",
    "with open(DB_TRIPLE_FILE) as my_triples:\n",
    "    for line in my_triples:\n",
    "        elements = parse_dbpedia_ontology(line)\n",
    "        if not elements:\n",
    "            continue\n",
    "        # add SUB-OBJ as nodes\n",
    "        db_graph.add_node(elements[0])\n",
    "        db_graph.add_node(elements[2])\n",
    "        # add edge\n",
    "        db_graph.add_edge(elements[0], elements[2])\n",
    "\n",
    "print(len(db_graph))\n",
    "db_page_rank = nx.pagerank(db_graph, max_iter=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the triple story to collect, for each target property, a collection of facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_collection = defaultdict(list)\n",
    "properties_running_count = defaultdict(set)\n",
    "with open(DB_LITERALS_FILE) as my_triples:\n",
    "    for line in my_triples:\n",
    "        elements = parse_dbpedia_triple(line)\n",
    "        if not elements or elements[0] not in uri2name or elements[1] not in TARGET_PROPERTIES:\n",
    "            continue\n",
    "        prop = elements[1]\n",
    "        property_owner = elements[0]\n",
    "        property_owner_label = uri2name[elements[0]]\n",
    "        property_value = parse_dbpedia_value(elements[2])\n",
    "        # reject if property value == 0 or owner already added\n",
    "        if property_value == 0.0 or property_owner in properties_running_count[prop]:\n",
    "            continue\n",
    "        # add to running count to avoid duplicates\n",
    "        properties_running_count[prop].add(property_owner)\n",
    "        property_meta = TARGET_PROPERTIES[prop]\n",
    "        property_final_value = property_value if not property_meta['t'] else property_meta['t'](property_value)\n",
    "        if property_meta['m'] and not property_meta['m'](property_final_value):\n",
    "            continue\n",
    "            \n",
    "        new_fact = {\n",
    "           'q': property_final_value,\n",
    "           'd': '{}({})'.format(property_meta['d'], property_owner_label),\n",
    "           'u': property_meta['u'],\n",
    "           'p': prop,\n",
    "           'r': db_page_rank.get(property_owner, 0.0)\n",
    "        }\n",
    "        # add to list\n",
    "        properties_collection[prop].append(new_fact)\n",
    "\n",
    "print(sum([len(p) for k, p in properties_collection.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(properties_collection['http://dbpedia.org/ontology/networth'], \n",
    "             key=lambda i: i['r'], \n",
    "             reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and Export\n",
    "Loop over the facts (as grouped by target property) and export subsets to be re-used in different WebPPL models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first dump all JSON\n",
    "with open(JSON_OUTPUT_FILE, 'w') as outfile:  \n",
    "    json.dump(properties_collection, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_mini_json(prop_collection, top_n):\n",
    "    # finally, for each property dumps only a subset to be used with simplified models \n",
    "    mini_properties_collection = {}\n",
    "    for p, p_list in prop_collection.items():\n",
    "        sorted_list = sorted(prop_collection[p], key=lambda i: i['r'], reverse=True)[:top_n]\n",
    "        mini_properties_collection[p] = sorted_list\n",
    "    all_facts = []\n",
    "    all_priors = []\n",
    "    for p, p_list in mini_properties_collection.items():\n",
    "        all_facts.extend(p_list)\n",
    "        all_priors.extend([log(len(p_list) - _) for _ in range(len(p_list))])\n",
    "        \n",
    "    categorical_distribution = {\n",
    "          'ps': all_priors,\n",
    "          'vs': [_ for _ in range(len(all_facts))]\n",
    "        }\n",
    "    with open('{}/distribution_top_{}.json'.format(DATA_FOLDER, top_n), 'w') as outfile:  \n",
    "        json.dump(categorical_distribution, outfile)\n",
    "    with open('{}/fact_metadata_top_{}.json'.format(DATA_FOLDER, top_n), 'w') as outfile:  \n",
    "        json.dump(all_facts, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_mini_json(properties_collection, 10)\n",
    "dump_mini_json(properties_collection, 15)\n",
    "dump_mini_json(properties_collection, 100)\n",
    "dump_mini_json(properties_collection, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
